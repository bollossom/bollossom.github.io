---
title: "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering"
collection: preprint
permalink: /publications/Pandora
excerpt: "Getting large language models (LLMs) to refuse to answer hostile toxicity questions is a core issue under the theme of LLMs security. Previous approaches have used prompts engineering to jailbreak LLMs and answer some toxicity questions. These approaches can easily fail after the model manufacturer makes additional fine-tuning to the model. To promote the further understanding of model jailbreaking by researchers, we are inspired by Representation Engineering to propose a jailbreaking method that does not require elaborate construction prompts, is not affected by model fine-tuning, and can be widely applied to any open-source LLMs in a pluggable manner. We have evaluated this method on multiple mainstream LLMs on carefully supplemented toxicity datasets, and the experimental results demonstrate the significant effectiveness of our approach. After being surprised by some interesting jailbreaking cases, we did extensive in-depth research to explore the techniques behind this method."
date: 2024-01-12
venue: "arxiv"
paperurl: "https://arxiv.org/pdf/2401.06824.pdf"
citation: "Li T, Zheng X, Huang X, et al. Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering[J]. arXiv preprint arXiv:2401.06824, 2024."
---

[Download paper here](https://arxiv.org/pdf/2401.06824.pdf)
